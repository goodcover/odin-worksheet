{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                          \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.odin.ExtractorEngine\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.processors.Processor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.processors.clu.CluProcessor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.processors.corenlp.CoreNLPProcessor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.processors.fastnlp.FastNLPProcessor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.processors.shallownlp.ShallowNLPProcessor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpprint.pprintln\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.clulab::processors-main:6.1.3`\n",
    "import $ivy.`org.clulab::processors-corenlp:6.1.3`\n",
    "import $ivy.`org.clulab::processors-odin:6.1.3`\n",
    "import $ivy.`org.clulab::processors-modelsmain:6.1.3`\n",
    "import $ivy.`org.clulab::processors-modelscorenlp:6.1.3`\n",
    "import $ivy.`com.lihaoyi::pprint:0.5.3`\n",
    "\n",
    "import org.clulab.odin.ExtractorEngine\n",
    "import org.clulab.processors.Processor\n",
    "import org.clulab.processors.clu.CluProcessor\n",
    "import org.clulab.processors.corenlp.CoreNLPProcessor\n",
    "import org.clulab.processors.fastnlp.FastNLPProcessor\n",
    "import org.clulab.processors.shallownlp.ShallowNLPProcessor\n",
    "import pprint.pprintln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.4 sec].\n",
      "Adding annotator lemma\n",
      "Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.1 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "sutime.binder.1.\n",
      "Initializing JollyDayHoliday for sutime with classpath:edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml\n",
      "Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt\n",
      "Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt\n",
      "Nov 14, 2017 3:20:58 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n",
      "INFO: Ignoring inactive rule: null\n",
      "Nov 14, 2017 3:20:58 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n",
      "INFO: Ignoring inactive rule: temporal-composite-8:ranges\n",
      "Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\n",
      "Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.2 sec].\n",
      "macro=true\n",
      "featureCountThresh=10\n",
      "featureFactory=org.clulab.processors.corenlp.chunker.ChunkingFeatureFactory\n",
      "Adding annotator dcoref\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mproc\u001b[39m: \u001b[32mProcessor\u001b[39m = org.clulab.processors.corenlp.CoreNLPProcessor@4f5f643c\n",
       "\u001b[36mtestPhrase\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello the world is not flat.\"\u001b[39m\n",
       "\u001b[36mdoc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mclulab\u001b[39m.\u001b[32mprocessors\u001b[39m.\u001b[32mDocument\u001b[39m = org.clulab.processors.corenlp.CoreNLPDocument@171b93f4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stanford one\n",
    "val proc: Processor = new CoreNLPProcessor()\n",
    "// Also Stanford but Neural Net based (faster).\n",
    "// val proc: Processor = new FastNLPProcessor()\n",
    "// CLU lab implementation (faster but less featureful)\n",
    "// val proc: Processor = new CluProcessor()\n",
    "val testPhrase = \"Hello the world is not flat.\"\n",
    "val doc = proc.annotate(testPhrase, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.odin.{EventMention, Mention, RelationMention, TextBoundMention}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.clulab.processors.{Document, Sentence}\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mNLPPrinter\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.clulab.odin.{EventMention, Mention, RelationMention, TextBoundMention}\n",
    "import org.clulab.processors.{Document, Sentence}\n",
    "\n",
    "object NLPPrinter {\n",
    "\n",
    "  def displayMentions(mentions: Seq[Mention], doc: Document): Unit = {\n",
    "    val mentionsBySentence = mentions groupBy (_.sentence) mapValues (_.sortBy(_.start)) withDefaultValue Nil\n",
    "    for ((s, i) <- doc.sentences.zipWithIndex) {\n",
    "      println(s\"sentence #$i\")\n",
    "      println(s.getSentenceText)\n",
    "      println(\"Tokens: \" + (s.words.indices, s.words, s.tags.get).zipped.mkString(\", \"))\n",
    "      printSyntacticDependencies(s)\n",
    "      println\n",
    "\n",
    "      val sortedMentions     = mentionsBySentence(i).sortBy(_.label)\n",
    "      val (events, entities) = sortedMentions.partition(_ matches \"Event\")\n",
    "      val (tbs, rels)        = entities.partition(_.isInstanceOf[TextBoundMention])\n",
    "      val sortedEntities     = tbs ++ rels.sortBy(_.label)\n",
    "      println(\"entities:\")\n",
    "      sortedEntities foreach displayMention\n",
    "\n",
    "      println\n",
    "      println(\"events:\")\n",
    "      events foreach displayMention\n",
    "      println(\"=\" * 50)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def printSyntacticDependencies(s: Sentence): Unit = {\n",
    "    if (s.dependencies.isDefined) {\n",
    "      println(s.dependencies.get.toString)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def displayMention(mention: Mention) {\n",
    "    val boundary = s\"\\t${\"-\" * 30}\"\n",
    "    println(s\"${mention.labels} => ${mention.text}\")\n",
    "    println(boundary)\n",
    "    println(s\"\\tRule => ${mention.foundBy}\")\n",
    "    val mentionType = mention.getClass.toString.split(\"\"\"\\.\"\"\").last\n",
    "    println(s\"\\tType => $mentionType\")\n",
    "    println(boundary)\n",
    "    mention match {\n",
    "      case tb: TextBoundMention =>\n",
    "        println(s\"\\t${tb.labels.mkString(\", \")} => ${tb.text}\")\n",
    "      case em: EventMention =>\n",
    "        println(s\"\\ttrigger => ${em.trigger.text}\")\n",
    "        displayArguments(em)\n",
    "      case rel: RelationMention =>\n",
    "        displayArguments(rel)\n",
    "      case _ => ()\n",
    "    }\n",
    "    println(s\"$boundary\\n\")\n",
    "  }\n",
    "\n",
    "  def displayArguments(b: Mention): Unit = {\n",
    "    b.arguments foreach {\n",
    "      case (argName, ms) =>\n",
    "        ms foreach { v =>\n",
    "          println(s\"\\t$argName ${v.labels.mkString(\"(\", \", \", \")\")} => ${v.text}\")\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence #0\n",
      "hello\n",
      "Tokens: (0,hello,UH)\n",
      "roots: 0\n",
      "outgoing:\n",
      "incoming:\n",
      "\n",
      "\n",
      "entities:\n",
      "\n",
      "events:\n",
      "List(greet, Intent, Event) => hello\n",
      "\t------------------------------\n",
      "\tRule => greet-2\n",
      "\tType => TextBoundMention\n",
      "\t------------------------------\n",
      "\tgreet, Intent, Event => hello\n",
      "\t------------------------------\n",
      "\n",
      "==================================================\n",
      "\u001b[33mSome\u001b[39m(\u001b[33mArray\u001b[39m(\u001b[32m\"O\"\u001b[39m))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrules\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"\n",
       "#resources:\n",
       "#  # use sample of w2v embeddings generated from gigaword\n",
       "#  embeddings: classpath:org/clulab/odin/embeddings/w2v-gigaword-sample.txt\n",
       "\n",
       "taxonomy:\n",
       "  - Entity:\n",
       "    - Subject\n",
       "    - Object\n",
       "    - PossiblePerson:\n",
       "      - Person\n",
       "      - Pronoun\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mextractor\u001b[39m: \u001b[32mExtractorEngine\u001b[39m = org.clulab.odin.ExtractorEngine@4730a1f1\n",
       "\u001b[36mphrase\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"hello\"\u001b[39m\n",
       "\u001b[36mdoc\u001b[39m: \u001b[32mDocument\u001b[39m = org.clulab.processors.corenlp.CoreNLPDocument@74f52e3c\n",
       "\u001b[36mmentions\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mMention\u001b[39m] = \u001b[33mVector\u001b[39m(org.clulab.odin.TextBoundMention@335de76e)\n",
       "\u001b[36mres4_7\u001b[39m: \u001b[32mOption\u001b[39m[\u001b[32mUnit\u001b[39m] = None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "val rules = scala.io.Source.fromFile(new File(\"../core/src/main/resources/grammars/claims/master.yml\")).mkString\n",
    "val extractor       = ExtractorEngine(rules)\n",
    "\n",
    "val phrase = \"hello\"\n",
    "val doc = proc.annotate(phrase)\n",
    "val mentions = extractor.extractFrom(doc)\n",
    "\n",
    "NLPPrinter.displayMentions(mentions, doc)\n",
    "mentions.find(_.label.contains(\"Date\")).map(f => pprintln(f.tokenInterval, height=Int.MaxValue))\n",
    "pprintln(doc.sentences(0).norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.typelevel::spire-extras:0.14.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspire.algebra._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspire.math._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspire.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mv\u001b[39m: \u001b[32mVector\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = \u001b[33mVector\u001b[39m(\n",
       "  (\u001b[32m\"coverage_query\"\u001b[39m, \u001b[32m0.2864827513694763\u001b[39m),\n",
       "  (\u001b[32m\"loss_cause\"\u001b[39m, \u001b[32m0.2397916465997696\u001b[39m),\n",
       "  (\u001b[32m\"billing_query\"\u001b[39m, \u001b[32m0.15513364970684052\u001b[39m),\n",
       "  (\u001b[32m\"policy_query\"\u001b[39m, \u001b[32m0.14402523636817932\u001b[39m),\n",
       "  (\u001b[32m\"policy_change_request\"\u001b[39m, \u001b[32m0.03895655646920204\u001b[39m),\n",
       "  (\u001b[32m\"bad_feedback\"\u001b[39m, \u001b[32m0.0363447479903698\u001b[39m),\n",
       "  (\u001b[32m\"greet\"\u001b[39m, \u001b[32m0.030883004888892174\u001b[39m),\n",
       "  (\u001b[32m\"initiate_complaint\"\u001b[39m, \u001b[32m0.028019487857818604\u001b[39m),\n",
       "  (\u001b[32m\"claim_initiate\"\u001b[39m, \u001b[32m0.026078691706061363\u001b[39m),\n",
       "  (\u001b[32m\"goodbye\"\u001b[39m, \u001b[32m0.014284259639680386\u001b[39m)\n",
       ")\n",
       "\u001b[36mh\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.2864827513694763\u001b[39m,\n",
       "  \u001b[32m0.2397916465997696\u001b[39m,\n",
       "  \u001b[32m0.15513364970684052\u001b[39m,\n",
       "  \u001b[32m0.14402523636817932\u001b[39m,\n",
       "  \u001b[32m0.03895655646920204\u001b[39m,\n",
       "  \u001b[32m0.0363447479903698\u001b[39m,\n",
       "  \u001b[32m0.030883004888892174\u001b[39m,\n",
       "  \u001b[32m0.028019487857818604\u001b[39m,\n",
       "  \u001b[32m0.026078691706061363\u001b[39m,\n",
       "  \u001b[32m0.014284259639680386\u001b[39m\n",
       ")\n",
       "\u001b[36mmean\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.10000000325962902\u001b[39m\n",
       "\u001b[36mmeanVector\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.1864827481098473\u001b[39m,\n",
       "  \u001b[32m0.13979164334014058\u001b[39m,\n",
       "  \u001b[32m0.0551336464472115\u001b[39m,\n",
       "  \u001b[32m0.044025233108550305\u001b[39m,\n",
       "  \u001b[32m-0.061043446790426975\u001b[39m,\n",
       "  \u001b[32m-0.06365525526925922\u001b[39m,\n",
       "  \u001b[32m-0.06911699837073684\u001b[39m,\n",
       "  \u001b[32m-0.07198051540181041\u001b[39m,\n",
       "  \u001b[32m-0.07392131155356765\u001b[39m,\n",
       "  \u001b[32m-0.08571574361994863\u001b[39m\n",
       ")\n",
       "\u001b[36mres4_7\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.09478589346417929\u001b[39m\n",
       "\u001b[36mres4_8\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.31622776601683794\u001b[39m\n",
       "\u001b[36mres4_9\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.09478589346417927\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spire.algebra._\n",
    "import spire.math._\n",
    "import spire.implicits._\n",
    "\n",
    "val v = Vector(\n",
    "  (\"coverage_query\", 0.2864827513694763),\n",
    "  (\"loss_cause\", 0.2397916465997696),\n",
    "  (\"billing_query\", 0.15513364970684052),\n",
    "  (\"policy_query\", 0.14402523636817932),\n",
    "  (\"policy_change_request\", 0.03895655646920204),\n",
    "  (\"bad_feedback\", 0.0363447479903698),\n",
    "  (\"greet\", 0.030883004888892174),\n",
    "  (\"initiate_complaint\", 0.028019487857818604),\n",
    "  (\"claim_initiate\", 0.026078691706061363),\n",
    "  (\"goodbye\", 0.014284259639680386)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "// def stdDev[T: Numeric](a: Vector[T]): T = {\n",
    "//     a.normalize.map(_.pow(2)).qsum/a.length\n",
    "// }\n",
    "\n",
    "// v.map(_._2).normalize.map(_.pow(2)).qsum/v.length\n",
    "val h = v.map(_._2)\n",
    "val mean = h.sum/h.length\n",
    "val meanVector = h.map(_ - mean)\n",
    "(h.map( _ - mean).map(t => t*t).sum/h.length).sqrt\n",
    "(h.normalize.map(_.pow(2)).qsum/h.length).sqrt\n",
    "h.map(_ - mean).map(_.pow(2)).qmean.sqrt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
